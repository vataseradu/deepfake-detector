{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f9a631",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ‚ö†Ô∏è SCHIMBƒÇ CALEA CƒÇTRE FOLDERUL CU DATASET-UL PE GOOGLE DRIVE\n",
    "DATASET_FOLDER = '/content/drive/My Drive/deepfake_dataset'\n",
    "\n",
    "# VerificƒÉ dacƒÉ existƒÉ ZIP-uri »ôi extrage-le\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "print(f\"üìÅ Con»õinut folder: {os.listdir(DATASET_FOLDER)}\")\n",
    "\n",
    "# CautƒÉ fi»ôiere ZIP\n",
    "zip_files = [f for f in os.listdir(DATASET_FOLDER) if f.endswith('.zip')]\n",
    "\n",
    "if zip_files:\n",
    "    print(f\"\\nüì¶ GƒÉsite {len(zip_files)} fi»ôiere ZIP:\")\n",
    "    for zip_file in zip_files:\n",
    "        zip_path = os.path.join(DATASET_FOLDER, zip_file)\n",
    "        print(f\"   Extrag: {zip_file}...\")\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DATASET_FOLDER)\n",
    "        \n",
    "        print(f\"   ‚úÖ Extras: {zip_file}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Con»õinut dupƒÉ extragere: {os.listdir(DATASET_FOLDER)}\")\n",
    "\n",
    "# VerificƒÉ structura finalƒÉ\n",
    "DATASET_PATH = os.path.join(DATASET_FOLDER, 'train')\n",
    "\n",
    "# DacƒÉ train/ nu existƒÉ, verificƒÉ dacƒÉ real/ »ôi fake/ sunt direct √Æn folder\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"\\n‚ö†Ô∏è Folder 'train/' nu gƒÉsit, verific dacƒÉ real/fake sunt direct √Æn {DATASET_FOLDER}\")\n",
    "    if os.path.exists(os.path.join(DATASET_FOLDER, 'real')) and os.path.exists(os.path.join(DATASET_FOLDER, 'fake')):\n",
    "        DATASET_PATH = DATASET_FOLDER\n",
    "        print(f\"‚úÖ GƒÉsite real/ »ôi fake/ direct √Æn {DATASET_FOLDER}\")\n",
    "\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"\\n‚úÖ Dataset final: {DATASET_PATH}\")\n",
    "    real_path = os.path.join(DATASET_PATH, 'real')\n",
    "    fake_path = os.path.join(DATASET_PATH, 'fake')\n",
    "    \n",
    "    if os.path.exists(real_path) and os.path.exists(fake_path):\n",
    "        real_count = len(os.listdir(real_path))\n",
    "        fake_count = len(os.listdir(fake_path))\n",
    "        print(f\"   Real: {real_count:,} imagini\")\n",
    "        print(f\"   Fake: {fake_count:,} imagini\")\n",
    "        print(f\"   Total: {real_count + fake_count:,} imagini\")\n",
    "    else:\n",
    "        print(f\"‚ùå Folderele real/ sau fake/ nu existƒÉ √Æn {DATASET_PATH}\")\n",
    "        print(f\"   StructurƒÉ gƒÉsitƒÉ: {os.listdir(DATASET_PATH)}\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset NU gƒÉsit. StructurƒÉ folder:\")\n",
    "    print(f\"   {os.listdir(DATASET_FOLDER)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc9e32",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ VerificƒÉ GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d349ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f\"\\n‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úÖ GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b45d01",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "print(\"‚úÖ Toate librƒÉriile importate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b692f9",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Configurare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e07d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'input_shape': (256, 256, 3),\n",
    "    'batch_size': 32,  # GPU T4 poate 32, vs 16 pe CPU\n",
    "    'dropout_rate': 0.5,\n",
    "    'train_split': 0.8,\n",
    "    \n",
    "    # Training phases\n",
    "    'initial_epochs': 10,\n",
    "    'fine_tune_epochs': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'fine_tune_layers': 20,\n",
    "    \n",
    "    # Data augmentation\n",
    "    'augmentation': {\n",
    "        'rotation_range': 20,\n",
    "        'width_shift_range': 0.2,\n",
    "        'height_shift_range': 0.2,\n",
    "        'horizontal_flip': True,\n",
    "        'zoom_range': 0.2,\n",
    "        'brightness_range': [0.8, 1.2],\n",
    "        'shear_range': 0.15,\n",
    "        'fill_mode': 'nearest'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configurare setatƒÉ\")\n",
    "print(f\"   Input shape: {CONFIG['input_shape']}\")\n",
    "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"   Total epochs: {CONFIG['initial_epochs'] + CONFIG['fine_tune_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347d5f2",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8fbad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data (with augmentation)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # ‚ö†Ô∏è IMPORTANT: Normalize 0-255 to 0-1\n",
    "    validation_split=1-CONFIG['train_split'],\n",
    "    **CONFIG['augmentation']\n",
    ")\n",
    "\n",
    "# Validation data (no augmentation)\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=1-CONFIG['train_split']\n",
    ")\n",
    "\n",
    "# Training generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=CONFIG['input_shape'][:2],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Validation generator\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=CONFIG['input_shape'][:2],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Training samples: {train_generator.samples:,}\")\n",
    "print(f\"‚úÖ Validation samples: {val_generator.samples:,}\")\n",
    "print(f\"‚úÖ Steps per epoch: {len(train_generator):,}\")\n",
    "print(f\"\\nClass indices: {train_generator.class_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9940d857",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Create Xception Model (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Load Xception pre-trained on ImageNet\n",
    "    base_model = Xception(\n",
    "        include_top=False,  # Remove classification head\n",
    "        weights='imagenet',  # Use pre-trained weights\n",
    "        input_shape=CONFIG['input_shape']\n",
    "    )\n",
    "    \n",
    "    # Freeze base model (Phase 1)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build complete model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(CONFIG['dropout_rate']),\n",
    "        layers.Dense(1, activation='sigmoid')  # Binary: 0=Real, 1=Fake\n",
    "    ], name='xception_deepfake_detector')\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Create model\n",
    "model, base_model = create_model()\n",
    "\n",
    "# Compile for Phase 1\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(CONFIG['learning_rate']),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "model.summary()\n",
    "\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.size(var).numpy() for var in model.trainable_variables])\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c85ad",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ PHASE 1: Train Classification Head (Frozen Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cd705",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: TRAINING CLASSIFICATION HEAD (BASE FROZEN)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {CONFIG['initial_epochs']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Callbacks\n",
    "callbacks_phase1 = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='/content/drive/My Drive/deepfake_phase1_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train Phase 1\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=CONFIG['initial_epochs'],\n",
    "    callbacks=callbacks_phase1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 1 completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0b5ed",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ PHASE 2: Fine-Tune Top Layers (Unfreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e521a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: FINE-TUNING (UNFREEZE TOP LAYERS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Unfreeze top layers\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-CONFIG['fine_tune_layers']]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"‚úÖ Unfroze last {CONFIG['fine_tune_layers']} layers\")\n",
    "trainable_params = sum([tf.size(var).numpy() for var in model.trainable_variables])\n",
    "print(f\"   Trainable params: {trainable_params:,}\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(CONFIG['learning_rate'] / 10),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nEpochs: {CONFIG['fine_tune_epochs']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate'] / 10}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Callbacks Phase 2\n",
    "callbacks_phase2 = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='/content/drive/My Drive/deepfake_xception_final.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train Phase 2\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=CONFIG['fine_tune_epochs'],\n",
    "    callbacks=callbacks_phase2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 2 completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904dfdda",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Evaluate Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = model.evaluate(val_generator, verbose=1)\n",
    "\n",
    "print(f\"\\nüìä Final Results:\")\n",
    "print(f\"   Loss: {results[0]:.4f}\")\n",
    "print(f\"   Accuracy: {results[1]*100:.2f}%\")\n",
    "print(f\"   Precision: {results[2]*100:.2f}%\")\n",
    "print(f\"   Recall: {results[3]*100:.2f}%\")\n",
    "print(f\"   AUC: {results[4]:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55979670",
   "metadata": {},
   "source": [
    "## üîü Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories\n",
    "history = {\n",
    "    'phase1': history_phase1.history,\n",
    "    'phase2': history_phase2.history\n",
    "}\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].plot(history_phase1.history['accuracy'], label='Phase 1 Train')\n",
    "axes[0, 0].plot(history_phase1.history['val_accuracy'], label='Phase 1 Val')\n",
    "axes[0, 0].plot(range(len(history_phase1.history['accuracy']), \n",
    "                      len(history_phase1.history['accuracy']) + len(history_phase2.history['accuracy'])),\n",
    "                history_phase2.history['accuracy'], label='Phase 2 Train')\n",
    "axes[0, 0].plot(range(len(history_phase1.history['accuracy']), \n",
    "                      len(history_phase1.history['accuracy']) + len(history_phase2.history['accuracy'])),\n",
    "                history_phase2.history['val_accuracy'], label='Phase 2 Val')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Training & Validation Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(history_phase1.history['loss'], label='Phase 1 Train')\n",
    "axes[0, 1].plot(history_phase1.history['val_loss'], label='Phase 1 Val')\n",
    "axes[0, 1].plot(range(len(history_phase1.history['loss']), \n",
    "                      len(history_phase1.history['loss']) + len(history_phase2.history['loss'])),\n",
    "                history_phase2.history['loss'], label='Phase 2 Train')\n",
    "axes[0, 1].plot(range(len(history_phase1.history['loss']), \n",
    "                      len(history_phase1.history['loss']) + len(history_phase2.history['loss'])),\n",
    "                history_phase2.history['val_loss'], label='Phase 2 Val')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_title('Training & Validation Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history_phase1.history['precision'], label='Phase 1 Train')\n",
    "axes[1, 0].plot(history_phase2.history['precision'], label='Phase 2 Train')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].set_title('Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history_phase1.history['recall'], label='Phase 1 Train')\n",
    "axes[1, 1].plot(history_phase2.history['recall'], label='Phase 2 Train')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].set_title('Recall')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/My Drive/training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Grafice salvate pe Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163b827",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'model_type': 'Transfer Learning - Xception',\n",
    "    'dataset': '140k Real and Fake Faces',\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'training_platform': 'Google Colab GPU T4',\n",
    "    'architecture': 'xception',\n",
    "    'input_shape': CONFIG['input_shape'],\n",
    "    'batch_size': CONFIG['batch_size'],\n",
    "    'initial_epochs': CONFIG['initial_epochs'],\n",
    "    'fine_tune_epochs': CONFIG['fine_tune_epochs'],\n",
    "    'total_epochs': CONFIG['initial_epochs'] + CONFIG['fine_tune_epochs'],\n",
    "    'training_samples': train_generator.samples,\n",
    "    'validation_samples': val_generator.samples,\n",
    "    'class_names': {v: k for k, v in train_generator.class_indices.items()},\n",
    "    'final_metrics': {\n",
    "        'loss': float(results[0]),\n",
    "        'accuracy': float(results[1]),\n",
    "        'precision': float(results[2]),\n",
    "        'recall': float(results[3]),\n",
    "        'auc': float(results[4])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to Google Drive\n",
    "with open('/content/drive/My Drive/deepfake_xception_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Metadata salvat pe Google Drive\")\n",
    "print(f\"\\nüìÅ Fi»ôiere salvate:\")\n",
    "print(f\"   1. /content/drive/My Drive/deepfake_xception_final.keras\")\n",
    "print(f\"   2. /content/drive/My Drive/deepfake_xception_metadata.json\")\n",
    "print(f\"   3. /content/drive/My Drive/training_history.png\")\n",
    "print(f\"\\nüéâ TRAINING COMPLET! DescarcƒÉ modelul de pe Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de91482",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Download Model (Op»õional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7981cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download direct √Æn browser (dacƒÉ vrei)\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• DescarcƒÉ fi»ôierele...\")\n",
    "files.download('/content/drive/My Drive/deepfake_xception_final.keras')\n",
    "files.download('/content/drive/My Drive/deepfake_xception_metadata.json')\n",
    "files.download('/content/drive/My Drive/training_history.png')\n",
    "print(\"‚úÖ Download complet!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
